# this script compares pre-tuned and post-tuned models

# import libraries
import pandas as pd
from sklearn.preprocessing import StandardScaler

# read in training and validation data
X_train = pd.read_excel('X_train.xlsx')
X_val = pd.read_excel('X_val.xlsx')
y_train = pd.read_excel('y_train.xlsx')
y_val = pd.read_excel('y_val.xlsx')

# scale the features using StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

################### Baseline Model #####################
import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score

# calculate the mean value of the target variable in the training data
mean_target = np.mean(y_train)

# predict the mean value for every input in the validation data
y_pred = np.full_like(y_val, mean_target)

# evaluate the performance of the mean baseline model using the mean squared error (MSE)
mse = mean_squared_error(y_val, y_pred)
r2 = r2_score(y_val, y_pred)

# print the MSE
print(f"Mean squared error of the mean baseline model: {mse:.4f}")
print(f"R2 Score of Baseline Model: {r2}")

baseline_mse = mse
###################### AdaBoost ########################
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_squared_error

# Train an AdaBoost Regressor on the training data
ada_reg = AdaBoostRegressor()
ada_reg.fit(X_train, y_train)

# Make predictions on the validation data
y_pred = ada_reg.predict(X_val)

# Calculate the mean squared error
mse = mean_squared_error(y_val, y_pred)
mae = mean_absolute_error(y_val, y_pred)
r2 = r2_score(y_val, y_pred)

print('AdaBoost Mean Squared Error:', mse)
print(f"R2 Score of AdaBoost: {r2}")

adaboost_mse = mse
###################### ElasticNet ####################
from sklearn.linear_model import ElasticNet

# Train an ElasticNet Regressor on the training data
en_reg = ElasticNet()
en_reg.fit(X_train, y_train)

# Make predictions on the validation data
y_pred = en_reg.predict(X_val)

# Calculate the mean squared error
mse = mean_squared_error(y_val, y_pred)
mae = mean_absolute_error(y_val, y_pred)
r2 = r2_score(y_val, y_pred)

print('ElasticNet Mean Squared Error:', mse)
print(f"R2 Score of ElasticNet Model: {r2}")

elasticnet_mse = mse
############################## Gradient Boosting #############
from sklearn.ensemble import GradientBoostingRegressor

# Train a Gradient Boosting Regressor model
model = GradientBoostingRegressor(random_state=42)
model.fit(X_train, y_train)

# Make predictions on the validation set set
y_pred = model.predict(X_val)

# Calculate the mean squared error
mse = mean_squared_error(y_val, y_pred)
r2 = r2_score(y_val, y_pred)

print(f"Gradient Boosting Mean Squared Error: {mse:.4f}")
print(f"R2 Score of Gradient Boost Model: {r2}")

gradient_boost_mse = mse
#################### Multi Layer Perceptron Neural Network ##################
from sklearn.neural_network import MLPRegressor

# Train a Neural Network Regressor model
model = MLPRegressor(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the validation set
y_pred = model.predict(X_val)

# Calculate the mean squared error
mse = mean_squared_error(y_val, y_pred)
mae = mean_absolute_error(y_val, y_pred)
r2 = r2_score(y_val, y_pred)

print(f"Multi Layer Perceptron Mean Squared Error: {mse:.4f}")
print(f"R2 Score of MLP Model: {r2}")

multi_layer_perceptron_mse = mse
########################### Recurrent Neural Network ####################
from keras.models import Sequential
from keras.layers import Dense, LSTM

# read in training and validation data
X_train = pd.read_excel('X_train.xlsx')
X_val = pd.read_excel('X_val.xlsx')
y_train = pd.read_excel('y_train.xlsx')
y_val = pd.read_excel('y_val.xlsx')

# scale the features using StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Reshape the input data to be 3D for LSTM layer
X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))

# Define the model architecture
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(1, X_train.shape[2])))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

# Train the model on the training data
model.fit(X_train, y_train, epochs=100, batch_size=32)

# Make predictions on the validation data
y_pred = model.predict(X_val)

# Calculate the mean squared error
mse = mean_squared_error(y_val, y_pred)
mae = mean_absolute_error(y_val, y_pred)
r2 = r2_score(y_val, y_pred)

print('Recurrent Neural Network Mean Squared Error:', mse)
print(f"R2 Score of RNN Model: {r2}")

recurrent_neural_network_mse = mse
########################## XGBoost #######################
import xgboost as xgb

# read in training and validation data
X_train = pd.read_excel('X_train.xlsx')
X_val = pd.read_excel('X_val.xlsx')
y_train = pd.read_excel('y_train.xlsx')
y_val = pd.read_excel('y_val.xlsx')

# scale the features using StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

xgb_reg = xgb.XGBRegressor()
xgb_reg.fit(X_train, y_train)

# Make predictions on the validation data
y_pred = xgb_reg.predict(X_val)

# Calculate the mean squared error
mse = mean_squared_error(y_val, y_pred)
r2 = r2_score(y_val, y_pred)

print('XGBoost Mean Squared Error:', mse)
print(f"R2 Score of XGBoost Model: {r2}")

xg_boost_mse = mse
########################### Plot Results #########################
import matplotlib.pyplot as plt

# # Define data
models = ['Baseline', 'Adaboost', 'ElasticNet', 'Gradient Boost', 'MLP', 'RNN', 'XGBoost']
mse_scores = [baseline_mse, adaboost_mse, elasticnet_mse, gradient_boost_mse, multi_layer_perceptron_mse, recurrent_neural_network_mse, xg_boost_mse]

# MSE values following hyperparameter tuning
adaboost_tuned_mse = 2951.80
elasticnet_tuned_mse = 3409.72
gradient_boost_tuned_mse = 2072.89
mlp_tuned_mse = 2854.25
rnn_tuned_mse = 3205.92
xg_boost_tuned_mse = 2345.25

# after tuning
# Plot MSE scores
plt.figure(figsize=(8, 8))
plt.plot(models, mse_scores, 'ro', label='MSE (Baseline)', markersize=10)
plt.plot(models[1], adaboost_mse, 'bo', label='MSE (Trained Models)', markersize=10)
plt.plot(models[2], elasticnet_mse, 'bo', markersize=10)
plt.plot(models[3], gradient_boost_mse, 'bo', markersize=10)
plt.plot(models[4], multi_layer_perceptron_mse, 'bo', markersize=10)
plt.plot(models[5], recurrent_neural_network_mse, 'bo', markersize=10)
plt.plot(models[6], xg_boost_mse, 'bo', markersize=10)
plt.plot([models[0], models[-1]], [baseline_mse, baseline_mse], 'k--', label='MSE Baseline Benchmark')
plt.ylabel('Mean Squared Error')
plt.title("Impact of Hyperparameter Tuning on MSE")
plt.xticks(rotation=30, ha='right')
plt.gca().yaxis.grid(True)
for i in range(len(models)):
    if i == 0:
        i = i + 1
    plt.text(models[i], mse_scores[i]+100, f'{mse_scores[i]:.3f}', ha='center', fontsize=10, color='b')
plt.plot(models[1], adaboost_tuned_mse, 'g*', label='MSE (Tuned Models)', markersize=10)
plt.plot(models[2], elasticnet_tuned_mse, 'g*', markersize=10)
plt.plot(models[3], gradient_boost_tuned_mse, 'g*', markersize=10)
plt.plot(models[4], mlp_tuned_mse, 'g*', markersize=10)
plt.plot(models[5], rnn_tuned_mse, 'g*', markersize=10)
plt.plot(models[6], xg_boost_tuned_mse, 'g*', markersize=10)
plt.text(models[1], adaboost_tuned_mse - 200, f'{adaboost_tuned_mse:.3f}', ha='center', fontsize=10, color='g')
plt.text(models[2], elasticnet_tuned_mse-200, f'{elasticnet_tuned_mse:.3f}', ha='center', fontsize=10, color='g')
plt.text(models[3], gradient_boost_tuned_mse-200, f'{gradient_boost_tuned_mse:.3f}', ha='center', fontsize=10, color='g')
plt.text(models[4], mlp_tuned_mse-200, f'{mlp_tuned_mse:.3f}', ha='center', fontsize=10, color='g')
plt.text(models[5], rnn_tuned_mse-200, f'{rnn_tuned_mse:.3f}', ha='center', fontsize=10, color='g')
plt.text(models[6], xg_boost_tuned_mse-200, f'{xg_boost_tuned_mse:.3f}', ha='center', fontsize=10, color='g')
plt.legend(loc='upper right')
plt.show()
