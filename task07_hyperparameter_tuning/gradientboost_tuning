# this script performs hyperparameter tuning on the Gradient Boosting model with regularization

# import libraries
from sklearn.metrics import mean_squared_error, r2_score
from itertools import product
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import pickle

from sklearn.ensemble import GradientBoostingRegressor

# Train a Gradient Boosting Regressor on the training data
gb_reg = GradientBoostingRegressor()

# read in training and validation data
X_train = pd.read_excel('X_train.xlsx')
X_val = pd.read_excel('X_val.xlsx')
y_train = pd.read_excel('y_train.xlsx')
y_val = pd.read_excel('y_val.xlsx')

# scale the features using StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# tune hyperparameters
# Define the hyperparameter grid
param_grid = {
        'learning_rate': [0.1, 0.2, 0.4, 0.6],
        'n_estimators': [5, 10, 20, 30],
        'max_depth': [8, 10, 15, 20],
        'min_samples_split': [4, 8, 12, 16, 20],
        'min_samples_leaf': [2, 3, 4],
}

# Initialize variables to keep track of the minimum MSE and its corresponding hyperparameters
min_mse = float('inf')
optimized_r2 = 0
min_learning_rate = None
min_n_estimators = None
min_max_depth = None
min_min_samples_split = None
min_min_samples_leaf = None

# Create a list of all possible combinations of hyperparameters
hyperparam_combinations = list(product(param_grid['learning_rate'], param_grid['n_estimators'],
                                        param_grid['max_depth'], param_grid['min_samples_split'], param_grid['min_samples_leaf']))

# Iterate over each hyperparameter combination and train the model
for learning_rate, n_estimators, max_depth, min_samples_split, min_samples_leaf in hyperparam_combinations:
    gb_reg.set_params(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth,
                        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)
    gb_reg.fit(X_train, y_train)
    y_pred = gb_reg.predict(X_val)
    mse = mean_squared_error(y_val, y_pred)
    r2 = r2_score(y_val, y_pred)
    print(f"learning_rate={learning_rate}, n_estimators={n_estimators}, max_depth={max_depth}, min_samples_split={min_samples_split}, min_samples_leaf={min_samples_leaf} -- Validation Mean Squared Error: {mse}")
    # Update the minimum MSE and its corresponding hyperparameters if a new minimum is found
    if mse < min_mse:
        min_mse = mse
        optimized_r2 = r2
        min_learning_rate = learning_rate
        min_n_estimators = n_estimators
        min_max_depth = max_depth
        min_min_samples_split = min_samples_split
        min_min_samples_leaf = min_samples_leaf

# Print the hyperparameters that give the minimum MSE
print(f"Minimum Mean Squared Error: {min_mse}")
print(f"R2 Score of Optimized Model: {optimized_r2}")
print(f"Corresponding Hyperparameters: learning_rate={min_learning_rate}, n_estimators={min_n_estimators}, max_depth={min_max_depth}, min_samples_split={min_min_samples_split}, min_samples_leaf={min_min_samples_leaf}")

# Save the model to a file
with open('Gradient_Boost_optimized.pkl', 'wb') as f:
    pickle.dump(gb_reg, f)

# Load the saved model from a file
with open('Gradient_Boost_optimized.pkl', 'rb') as f:
    model = pickle.load(f)
