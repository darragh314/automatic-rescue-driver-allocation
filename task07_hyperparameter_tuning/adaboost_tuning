# this script performs hyperparameter tuning on the AdaBoost model

# import libraries
from sklearn.metrics import mean_squared_error, r2_score
from itertools import product
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import pickle

from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

# create AdaBoost model
base_model = DecisionTreeRegressor(max_depth=1)
ada_reg = AdaBoostRegressor(base_estimator=base_model)

# read in training and validation data
X_train = pd.read_excel('X_train.xlsx')
X_val = pd.read_excel('X_val.xlsx')
y_train = pd.read_excel('y_train.xlsx')
y_val = pd.read_excel('y_val.xlsx')

# scale the features using StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# tune hyperparameters
# Define the hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 1.0, 1.5],
    'base_estimator__max_depth': [1, 2, 3, 5],
}

# Initialize variables to keep track of the minimum MSE and its corresponding hyperparameters
min_mse = float('inf')
optimized_r2 = 0
min_n_estimators = None
min_learning_rate = None
min_base_estimator__max_depth = None

# Create a list of all possible combinations of hyperparameters
hyperparam_combinations = list(product(param_grid['n_estimators'], param_grid['learning_rate'],
                                        param_grid['base_estimator__max_depth']))

# Iterate over each hyperparameter combination and train the model
for n_estimators, learning_rate, base_estimator__max_depth in hyperparam_combinations:
    base_model = DecisionTreeRegressor(max_depth=base_estimator__max_depth)
    ada_reg = AdaBoostRegressor(base_estimator=base_model, n_estimators=n_estimators, learning_rate=learning_rate)
    ada_reg.fit(X_train, y_train)
    y_pred = ada_reg.predict(X_val)
    mse = mean_squared_error(y_val, y_pred)
    r2 = r2_score(y_val, y_pred)
    print(f"n_estimators={n_estimators}, learning_rate={learning_rate}, "
          f"base_estimator__max_depth={base_estimator__max_depth},"
          f"-- Validation Mean Squared Error: {mse}")
    # Update the minimum MSE and its corresponding hyperparameters if a new minimum is found
    if mse < min_mse:
        min_mse = mse
        optimized_r2 = r2
        min_n_estimators = n_estimators
        min_learning_rate = learning_rate
        min_base_estimator__max_depth = base_estimator__max_depth

# Print the hyperparameters that give the minimum MSE
print(f"Minimum Mean Squared Error: {min_mse}")
print(f"R2 Score of Optimized Model: {optimized_r2}")
print(f"Corresponding Hyperparameters: n_estimators={min_n_estimators}, learning_rate={min_learning_rate}, "
      f"base_estimator__max_depth={min_base_estimator__max_depth}")

# Save the model to a file
with open('AdaBoost_optimized.pkl', 'wb') as f:
    pickle.dump(ada_reg, f)

# Check that we can load the saved model from a file
with open('AdaBoost_optimized.pkl', 'rb') as f:
    model = pickle.load(f)
